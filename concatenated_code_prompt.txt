
==============================
# Begin file: pmbrl/__init__.py
from . import envs
from . import control
from . import envs
from . import training
from . import utils
from .configs import *
# End file: pmbrl/__init__.py

==============================

==============================
# Begin file: pmbrl/training/buffer.py
import torch
import numpy as np

class Buffer(object):
    def __init__(
        self,
        state_size,
        action_size,
        ensemble_size,
        normalizer,
        signal_noise=None,
        buffer_size=10 ** 6,
        device="cpu",
    ):
        self.state_size = state_size
        self.action_size = action_size
        self.ensemble_size = ensemble_size
        self.buffer_size = buffer_size
        self.signal_noise = signal_noise
        self.device = device

        self.states = np.zeros((buffer_size, state_size))
        self.actions = np.zeros((buffer_size, action_size))
        self.rewards = np.zeros((buffer_size, 1))
        self.state_deltas = np.zeros((buffer_size, state_size))

        self.normalizer = normalizer
        self._total_steps = 0

    def add(self, state, action, reward, next_state):
        idx = self._total_steps % self.buffer_size
        state_delta = next_state - state

        self.states[idx] = state
        self.actions[idx] = action
        self.rewards[idx] = reward
        self.state_deltas[idx] = state_delta
        self._total_steps += 1

        self.normalizer.update(state, action, state_delta)

    def get_train_batches(self, batch_size):
        size = len(self)
        indices = [
            np.random.permutation(range(size)) for _ in range(self.ensemble_size)
        ]
        indices = np.stack(indices).T

        for i in range(0, size, batch_size):
            j = min(size, i + batch_size)

            if (j - i) < batch_size and i != 0:
                return

            batch_size_i = j - i

            batch_indices = indices[i:j]
            batch_indices = batch_indices.flatten()

            states = self.states[batch_indices]
            actions = self.actions[batch_indices]
            rewards = self.rewards[batch_indices]
            state_deltas = self.state_deltas[batch_indices]

            states = torch.from_numpy(states).float().to(self.device)
            actions = torch.from_numpy(actions).float().to(self.device)
            rewards = torch.from_numpy(rewards).float().to(self.device)
            state_deltas = torch.from_numpy(state_deltas).float().to(self.device)

            if self.signal_noise is not None:
                states = states + self.signal_noise * torch.randn_like(states)

            states = states.reshape(self.ensemble_size, batch_size_i, self.state_size)
            actions = actions.reshape(self.ensemble_size, batch_size_i, self.action_size)
            rewards = rewards.reshape(self.ensemble_size, batch_size_i, 1)
            state_deltas = state_deltas.reshape(
                self.ensemble_size, batch_size_i, self.state_size
            )

            yield states, actions, rewards, state_deltas

    def __len__(self):
        return min(self._total_steps, self.buffer_size)

    @property
    def total_steps(self):
        return self._total_steps

# End file: pmbrl/training/buffer.py

==============================

==============================
# Begin file: pmbrl/training/trainer.py
# pylint: disable=not-callable
# pylint: disable=no-member

import torch

class Trainer(object):
    def __init__(
        self,
        ensemble,
        reward_model,
        buffer,
        n_train_epochs,
        batch_size,
        learning_rate,
        epsilon,
        grad_clip_norm,
        logger=None,
    ):
        self.ensemble = ensemble
        self.reward_model = reward_model
        self.buffer = buffer
        self.n_train_epochs = n_train_epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.grad_clip_norm = grad_clip_norm
        self.logger = logger

        self.params = (
            list(ensemble.parameters())
            + list(reward_model.parameters())
        )
        self.optim = torch.optim.Adam(self.params, lr=learning_rate, eps=epsilon)

    def train(self):
        e_losses = []
        r_losses = []
        n_batches = []
        for epoch in range(1, self.n_train_epochs + 1):
            e_losses.append([])
            r_losses.append([])
            n_batches.append(0)

            for (states, actions, rewards, deltas) in self.buffer.get_train_batches(
                self.batch_size
            ):
                self.ensemble.train()
                self.reward_model.train()

                self.optim.zero_grad()
                e_loss = self.ensemble.loss(states, actions, deltas)
                r_loss = self.reward_model.loss(states, actions, rewards)
                total_loss = e_loss + r_loss
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.params, self.grad_clip_norm, norm_type=2
                )
                self.optim.step()

                e_losses[epoch - 1].append(e_loss.item())
                r_losses[epoch - 1].append(r_loss.item())
                n_batches[epoch - 1] += 1

            if self.logger is not None and epoch % 20 == 0:
                avg_e_loss = self._get_avg_loss(e_losses, n_batches, epoch)
                avg_r_loss = self._get_avg_loss(r_losses, n_batches, epoch)
                message = "> Train epoch {} [ensemble {:.2f} | reward {:.2f}]"
                self.logger.log(message.format(epoch, avg_e_loss, avg_r_loss))

        return (
            self._get_avg_loss(e_losses, n_batches, epoch),
            self._get_avg_loss(r_losses, n_batches, epoch),
        )

    def reset_models(self):
        self.ensemble.reset_parameters()
        self.reward_model.reset_parameters()
        self.params = (
            list(self.ensemble.parameters())
            + list(self.reward_model.parameters())
        )
        self.optim = torch.optim.Adam(
            self.params, lr=self.learning_rate, eps=self.epsilon
        )

    def _get_avg_loss(self, losses, n_batches, epoch):
        epoch_loss = [sum(loss) / n_batch for loss, n_batch in zip(losses, n_batches)]
        return sum(epoch_loss) / epoch



# End file: pmbrl/training/trainer.py

==============================

==============================
# Begin file: pmbrl/training/normalizer.py
# pylint: disable=not-callable
# pylint: disable=no-member

import numpy as np
import torch


class Normalizer(object):
    def __init__(self):
        self.state_mean = None
        self.state_sk = None
        self.state_stdev = None
        self.action_mean = None
        self.action_sk = None
        self.action_stdev = None
        self.state_delta_mean = None
        self.state_delta_sk = None
        self.state_delta_stdev = None
        self.count = 0

    @staticmethod
    def update_mean(mu_old, addendum, n):
        mu_new = mu_old + (addendum - mu_old) / n
        return mu_new

    @staticmethod
    def update_sk(sk_old, mu_old, mu_new, addendum):
        sk_new = sk_old + (addendum - mu_old) * (addendum - mu_new)
        return sk_new

    def update(self, state, action, state_delta):
        self.count += 1

        if self.count == 1:
            self.state_mean = state.copy()
            self.state_sk = np.zeros_like(state)
            self.state_stdev = np.zeros_like(state)
            self.action_mean = action.copy()
            self.action_sk = np.zeros_like(action)
            self.action_stdev = np.zeros_like(action)
            self.state_delta_mean = state_delta.copy()
            self.state_delta_sk = np.zeros_like(state_delta)
            self.state_delta_stdev = np.zeros_like(state_delta)
            return

        state_mean_old = self.state_mean.copy()
        action_mean_old = self.action_mean.copy()
        state_delta_mean_old = self.state_delta_mean.copy()

        self.state_mean = self.update_mean(self.state_mean, state, self.count)
        self.action_mean = self.update_mean(self.action_mean, action, self.count)
        self.state_delta_mean = self.update_mean(
            self.state_delta_mean, state_delta, self.count
        )

        self.state_sk = self.update_sk(
            self.state_sk, state_mean_old, self.state_mean, state
        )
        self.action_sk = self.update_sk(
            self.action_sk, action_mean_old, self.action_mean, action
        )
        self.state_delta_sk = self.update_sk(
            self.state_delta_sk,
            state_delta_mean_old,
            self.state_delta_mean,
            state_delta,
        )

        self.state_stdev = np.sqrt(self.state_sk / self.count)
        self.action_stdev = np.sqrt(self.action_sk / self.count)
        self.state_delta_stdev = np.sqrt(self.state_delta_sk / self.count)

    @staticmethod
    def setup_vars(x, mean, stdev):
        mean, stdev = mean.copy(), stdev.copy()
        mean = torch.from_numpy(mean).float().to(x.device)
        stdev = torch.from_numpy(stdev).float().to(x.device)
        return mean, stdev

    def _normalize(self, x, mean, stdev):
        mean, stdev = self.setup_vars(x, mean, stdev)
        n = x - mean
        n = n / torch.clamp(stdev, min=1e-8)
        return n

    def normalize_states(self, states):
        return self._normalize(states, self.state_mean, self.state_stdev)

    def normalize_actions(self, actions):
        return self._normalize(actions, self.action_mean, self.action_stdev)

    def normalize_state_deltas(self, state_deltas):
        return self._normalize(
            state_deltas, self.state_delta_mean, self.state_delta_stdev
        )

    def denormalize_state_delta_means(self, state_deltas_means):
        mean, stdev = self.setup_vars(
            state_deltas_means, self.state_delta_mean, self.state_delta_stdev
        )
        return state_deltas_means * stdev + mean

    def denormalize_state_delta_vars(self, state_delta_vars):
        _, stdev = self.setup_vars(
            state_delta_vars, self.state_delta_mean, self.state_delta_stdev
        )
        return state_delta_vars * (stdev ** 2)

    def renormalize_state_delta_means(self, state_deltas_means):
        mean, stdev = self.setup_vars(
            state_deltas_means, self.state_delta_mean, self.state_delta_stdev
        )
        return (state_deltas_means - mean) / torch.clamp(stdev, min=1e-8)

    def renormalize_state_delta_vars(self, state_delta_vars):
        _, stdev = self.setup_vars(
            state_delta_vars, self.state_delta_mean, self.state_delta_stdev
        )
        return state_delta_vars / (torch.clamp(stdev, min=1e-8) ** 2)


# End file: pmbrl/training/normalizer.py

==============================

==============================
# Begin file: pmbrl/models/models.py
# pylint: disable=not-callable
# pylint: disable=no-member

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal


def swish(x):
    return x * torch.sigmoid(x)


class EnsembleDenseLayer(nn.Module):
    def __init__(self, in_size, out_size, ensemble_size, act_fn="swish"):
        super().__init__()
        self.in_size = in_size
        self.out_size = out_size
        self.ensemble_size = ensemble_size
        self.act_fn_name = act_fn
        self.act_fn = self._get_act_fn(self.act_fn_name)
        self.reset_parameters()

    def forward(self, x):
        op = torch.baddbmm(self.biases, x, self.weights)
        op = self.act_fn(op)
        return op

    def reset_parameters(self):
        weights = torch.zeros(self.ensemble_size, self.in_size, self.out_size).float()
        biases = torch.zeros(self.ensemble_size, 1, self.out_size).float()

        for weight in weights:
            self._init_weight(weight, self.act_fn_name)

        self.weights = nn.Parameter(weights)
        self.biases = nn.Parameter(biases)

    def _init_weight(self, weight, act_fn_name):
        if act_fn_name == "swish":
            nn.init.xavier_uniform_(weight)
        elif act_fn_name == "linear":
            nn.init.xavier_normal_(weight)

    def _get_act_fn(self, act_fn_name):
        if act_fn_name == "swish":
            return swish
        elif act_fn_name == "linear":
            return lambda x: x


class EnsembleModel(nn.Module):
    def __init__(
        self,
        in_size,
        out_size,
        hidden_size,
        ensemble_size,
        normalizer,
        act_fn="swish",
        device="cpu",
    ):
        super().__init__()

        self.fc_1 = EnsembleDenseLayer(
            in_size, hidden_size, ensemble_size, act_fn=act_fn
        )
        self.fc_2 = EnsembleDenseLayer(
            hidden_size, hidden_size, ensemble_size, act_fn=act_fn
        )
        self.fc_3 = EnsembleDenseLayer(
            hidden_size, hidden_size, ensemble_size, act_fn=act_fn
        )
        self.fc_4 = EnsembleDenseLayer(
            hidden_size, out_size * 2, ensemble_size, act_fn="linear"
        )

        self.ensemble_size = ensemble_size
        self.normalizer = normalizer
        self.device = device
        self.max_logvar = -1
        self.min_logvar = -5
        self.device = device
        self.to(device)

    def forward(self, states, actions):
        norm_states, norm_actions = self._pre_process_model_inputs(states, actions)
        norm_delta_mean, norm_delta_var = self._propagate_network(
            norm_states, norm_actions
        )
        delta_mean, delta_var = self._post_process_model_outputs(
            norm_delta_mean, norm_delta_var
        )
        return delta_mean, delta_var

    def loss(self, states, actions, state_deltas):
        states, actions = self._pre_process_model_inputs(states, actions)
        delta_targets = self._pre_process_model_targets(state_deltas)
        delta_mu, delta_var = self._propagate_network(states, actions)
        loss = (delta_mu - delta_targets) ** 2 / delta_var + torch.log(delta_var)
        loss = loss.mean(-1).mean(-1).sum()
        return loss

    def sample(self, mean, var):
        return Normal(mean, torch.sqrt(var)).sample()

    def reset_parameters(self):
        self.fc_1.reset_parameters()
        self.fc_2.reset_parameters()
        self.fc_3.reset_parameters()
        self.fc_4.reset_parameters()
        self.to(self.device)

    def _propagate_network(self, states, actions):
        inp = torch.cat((states, actions), dim=2)
        op = self.fc_1(inp)
        op = self.fc_2(op)
        op = self.fc_3(op)
        op = self.fc_4(op)

        delta_mean, delta_logvar = torch.split(op, op.size(2) // 2, dim=2)
        delta_logvar = torch.sigmoid(delta_logvar)
        delta_logvar = (
            self.min_logvar + (self.max_logvar - self.min_logvar) * delta_logvar
        )
        delta_var = torch.exp(delta_logvar)

        return delta_mean, delta_var

    def _pre_process_model_inputs(self, states, actions):
        states = states.to(self.device)
        actions = actions.to(self.device)
        states = self.normalizer.normalize_states(states)
        actions = self.normalizer.normalize_actions(actions)
        return states, actions

    def _pre_process_model_targets(self, state_deltas):
        state_deltas = state_deltas.to(self.device)
        state_deltas = self.normalizer.normalize_state_deltas(state_deltas)
        return state_deltas

    def _post_process_model_outputs(self, delta_mean, delta_var):
        delta_mean = self.normalizer.denormalize_state_delta_means(delta_mean)
        delta_var = self.normalizer.denormalize_state_delta_vars(delta_var)
        return delta_mean, delta_var


class RewardModel(nn.Module):
    def __init__(self, in_size, hidden_size, act_fn="relu", device="cpu"):
        super().__init__()
        self.in_size = in_size
        self.hidden_size = hidden_size
        self.device = device
        self.act_fn = getattr(F, act_fn)
        self.reset_parameters()
        self.to(device)

    def forward(self, states, actions):
        inp = torch.cat((states, actions), dim=-1)
        reward = self.act_fn(self.fc_1(inp))
        reward = self.act_fn(self.fc_2(reward))
        reward = self.fc_3(reward).squeeze(dim=1)
        return reward

    def loss(self, states, actions, rewards):
        r_hat = self(states, actions)
        return F.mse_loss(r_hat, rewards)

    def reset_parameters(self):
        self.fc_1 = nn.Linear(self.in_size, self.hidden_size)
        self.fc_2 = nn.Linear(self.hidden_size, self.hidden_size)
        self.fc_3 = nn.Linear(self.hidden_size, 1)
        self.to(self.device)

# End file: pmbrl/models/models.py

==============================

==============================
# Begin file: pmbrl/control/planner.py
# Begin file: pmbrl/control/planner.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal

from pmbrl.control.measures import InformationGain, Disagreement, Variance, Random

class Planner(nn.Module):
    def __init__(
        self,
        env,
        ensemble,
        reward_model,
        action_size,
        ensemble_size,
        plan_horizon,
        optimisation_iters,
        n_candidates,
        top_candidates,
        use_reward=True,
        use_exploration=True,
        use_mean=False,
        expl_scale=1.0,
        reward_scale=1.0,
        strategy="information",
        device="cpu",
        use_high_level=False,  # High-level planner flag
        context_length=1,  # Context length
        goal_achievement_scale=1.0,  # Scale for goal-achievement reward
        global_goal_state=None,  # The known global goal state
        # Additional parameters
        subgoal_scale=1.0,
        global_goal_scale=1.0,
        logger=None,  # Logger for debugging
        action_low=None,  # Lower bounds of action space
        action_high=None,  # Upper bounds of action space
        # New flags
        use_high_level_reward=True,
        use_high_level_exploration=True,
    ):
        super().__init__()
        self.env = env
        self.ensemble = ensemble
        self.reward_model = reward_model
        self.action_size = action_size
        self.ensemble_size = ensemble_size
        
        # self.use_goal_achievement = use_goal_achievement 

        self.plan_horizon = plan_horizon
        self.optimisation_iters = optimisation_iters
        self.n_candidates = n_candidates
        self.top_candidates = top_candidates

        self.use_reward = use_reward
        self.use_exploration = use_exploration
        self.use_mean = use_mean
        self.expl_scale = expl_scale
        self.reward_scale = reward_scale
        self.device = device

        self.use_high_level = use_high_level  # High-level planner flag
        self.context_length = context_length
        self.goal_achievement_scale = goal_achievement_scale
        self.global_goal_state = global_goal_state.to(device)  # Ensure on correct device

        # Additional parameters
        self.subgoal_scale = subgoal_scale
        self.global_goal_scale = global_goal_scale
        self.logger = logger  # Pass logger for debugging

        # Action bounds
        if self.env.action_space is None:
            raise ValueError("Action bounds (action_low and action_high) must be provided.")
        self.action_low = torch.tensor(self.env.action_space.low, dtype=torch.float32).to(device)
        self.action_high = torch.tensor(self.env.action_space.high, dtype=torch.float32).to(device)

        # High-level planner flags
        self.use_high_level_reward = use_high_level_reward
        self.use_high_level_exploration = use_high_level_exploration

        if strategy == "information":
            self.measure = InformationGain(self.ensemble, scale=expl_scale)
        elif strategy == "variance":
            self.measure = Variance(self.ensemble, scale=expl_scale)
        elif strategy == "random":
            self.measure = Random(self.ensemble, scale=expl_scale)
        elif strategy == "none":
            self.use_exploration = False

        self.trial_rewards = []
        self.trial_bonuses = []
        
        self.logger.log(f'exploration scale: {self.expl_scale}')
        self.logger.log(f'reward scale: {self.reward_scale}')
        # self.logger.log(f'subgoal scale: {self.subgoal_scale}')
        self.logger.log(f'global goal scale: {self.global_goal_scale}')
        self.logger.log(f'Goal achievement scale: {self.goal_achievement_scale}')
        self.to(device)

    def forward(self, state):
        state = torch.from_numpy(state).float().to(self.device)
        state_size = state.size(0)

        # Initialize action distributions
        action_mean = torch.zeros(self.plan_horizon, 1, self.action_size).to(self.device)
        action_std_dev = torch.ones(self.plan_horizon, 1, self.action_size).to(self.device)

        # Generate feasible subgoals using the dynamics model
        num_subgoals = (self.plan_horizon + self.context_length - 1) // self.context_length
        goal_size = state_size

        # Generate subgoals by simulating future states
        goal_mean = state.unsqueeze(0).repeat(self.ensemble_size, 1, 1)  # Shape: (ensemble_size, 1, state_size)
        if self.use_high_level:
            goal_mean = self.generate_feasible_subgoals(state, num_subgoals)

        # Select the current subgoal (e.g., the first one)
        current_subgoal = goal_mean[0]  # Shape: (state_size,)

        for iter in range(self.optimisation_iters):
            # Sample action candidates
            actions = action_mean + action_std_dev * torch.randn(
                self.plan_horizon,
                self.n_candidates,
                self.action_size,
                device=self.device,
            )

            # Clamp actions to respect environment bounds
            actions = self.clamp_actions(actions)

            # Perform rollouts with sampled actions
            states, delta_vars, delta_means = self.perform_rollout(state, actions)

            # Compute returns
            returns = torch.zeros(self.n_candidates).float().to(self.device)

            if self.use_reward:
                _states = states.view(-1, state_size)
                _actions = actions.unsqueeze(0).repeat(self.ensemble_size, 1, 1, 1)
                _actions = _actions.view(-1, self.action_size)
                rewards = self.reward_model(_states, _actions)
                rewards = rewards * self.reward_scale
                rewards = rewards.view(
                    self.plan_horizon, self.ensemble_size, self.n_candidates
                )
                rewards = rewards.mean(dim=1).sum(dim=0)
                returns += rewards
                self.trial_rewards.append(rewards)

            if self.use_exploration:
                expl_bonus = self.measure(delta_means, delta_vars) * self.expl_scale
                returns += expl_bonus
                self.trial_bonuses.append(expl_bonus)

            if self.use_high_level:
                goal_achievement = self.compute_goal_achievement(states, current_subgoal)* self.goal_achievement_scale
                returns += goal_achievement
            # else:
            #     # Add subgoal distance penalty
            #     subgoal_penalty = self.compute_subgoal_distance_penalty(states, current_subgoal)
            #     returns += subgoal_penalty

            # Update action distributions using top candidates
            action_mean, action_std_dev = self._update_action_distribution(
                actions, returns
            )

        # Extract the first action to return
        selected_action = action_mean[0].squeeze(dim=0)  # Shape: (action_size,)

        # For visualization, return the current subgoal
        selected_goal = current_subgoal  # Shape: (goal_size,)

        return selected_action, selected_goal

    def clamp_actions(self, actions):
        """
        Clamp the action sequences to respect the environment's action bounds.
        """
        # Expand action_low and action_high to match the actions' shape
        # actions: (plan_horizon, n_candidates, action_size)
        # action_low and action_high: (action_size,)
        # Need to reshape to (1, 1, action_size) for broadcasting
        action_low = self.action_low.view(1, 1, -1)
        action_high = self.action_high.view(1, 1, -1)
        return torch.clamp(actions, min=action_low, max=action_high)

    def generate_feasible_subgoals(self, current_state, num_subgoals):
        """
        Generate feasible subgoals by simulating future states using the dynamics model.
        """
        # Initialize
        n_subgoal_candidates = 1000  # Number of action sequences to sample for subgoal generation
        current_state = current_state.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, state_size)
        current_state = current_state.repeat(self.ensemble_size, n_subgoal_candidates, 1)  # Shape: (ensemble_size, n_subgoal_candidates, state_size)

        # Initialize action distributions for subgoal optimization
        goal_action_mean = torch.zeros(self.context_length, n_subgoal_candidates, self.action_size).to(self.device)
        goal_action_std_dev = torch.ones(self.context_length, n_subgoal_candidates, self.action_size).to(self.device)

        for iter in range(self.optimisation_iters):
            # Sample action sequences for subgoal candidates
            goal_actions = goal_action_mean + goal_action_std_dev * torch.randn(
                self.context_length,
                n_subgoal_candidates,
                self.action_size,
                device=self.device,
            )
            # Clamp actions
            goal_actions = self.clamp_actions(goal_actions)

            # Perform rollouts to get predicted subgoal states
            goal_states, goal_delta_vars, goal_delta_means = self.perform_subgoal_rollout(
                current_state, goal_actions
            )

            # Compute returns for subgoal candidates
            goal_returns = torch.zeros(n_subgoal_candidates).float().to(self.device)

            if self.use_high_level_reward:
                # Compute rewards for subgoal candidates
                _states = goal_states.view(-1, current_state.size(-1))
                _actions = goal_actions.unsqueeze(0).repeat(self.ensemble_size, 1, 1, 1)
                _actions = _actions.view(-1, self.action_size)
                rewards = self.reward_model(_states, _actions)
                # rewards = rewards * self.reward_scale
                rewards = rewards * 10.0
                rewards = rewards.view(
                    self.context_length, self.ensemble_size, n_subgoal_candidates
                )
                rewards = rewards.mean(dim=1).sum(dim=0)
                goal_returns += rewards

            if self.use_high_level_exploration:
                # Compute exploration bonuses for subgoal candidates
                expl_bonus = self.measure(goal_delta_means, goal_delta_vars) * self.expl_scale
                goal_returns += expl_bonus

            # Include distance to global goal (encourage subgoals closer to global goal)
            final_goal_states = goal_states[-1].mean(dim=0)  # Shape: (n_subgoal_candidates, state_size)
            distances_to_global_goal = torch.norm(final_goal_states - self.global_goal_state.unsqueeze(0), dim=1)
            goal_returns -= self.global_goal_scale * distances_to_global_goal

            # Update goal action distributions using top candidates
            goal_action_mean, goal_action_std_dev = self._update_goal_action_distribution(
                goal_actions, goal_returns
            )

        # After optimization, select the best action sequence based on the highest return
        best_return, best_index = goal_returns.max(dim=0)
        best_goal_actions = goal_actions[:, best_index, :].unsqueeze(1)  # Shape: (context_length, 1, action_size)
        current_state_best = current_state[:, best_index, :].unsqueeze(1)  # Shape: (ensemble_size, 1, state_size)

        # Perform rollout with the best actions to get subgoal state
        goal_states, _, _ = self.perform_subgoal_rollout(current_state_best, best_goal_actions)
        subgoal_state = goal_states[-1].mean(dim=0).squeeze(0)  # Shape: (state_size,)
        goal_mean = subgoal_state.unsqueeze(0)  # Shape: (1, state_size)

        # Repeat the subgoal for the number of subgoals needed
        goal_mean = goal_mean.repeat(num_subgoals, 1)  # Shape: (num_subgoals, state_size)

        return goal_mean

    def perform_subgoal_rollout(self, current_state, actions):
        """
        Perform rollout to predict future states for subgoal candidates.

        Args:
            current_state (torch.Tensor): The current state tensor, shape (ensemble_size, n_candidates, state_size)
            actions (torch.Tensor): Actions for subgoal candidates, shape (context_length, n_candidates, action_size)

        Returns:
            Tuple of predicted states, delta_vars, delta_means.
        """
        T = actions.size(0) + 1
        n_candidates = actions.size(1)
        states = [torch.empty(0)] * T
        delta_means = [torch.empty(0)] * (T - 1)
        delta_vars = [torch.empty(0)] * (T - 1)

        states[0] = current_state  # Shape: (ensemble_size, n_candidates, state_size)

        # Repeat actions for each ensemble member
        actions = actions.unsqueeze(1).repeat(1, self.ensemble_size, 1, 1)  # Shape: (context_length, ensemble_size, n_candidates, action_size)
        actions = actions.permute(1, 0, 2, 3)  # Shape: (ensemble_size, context_length, n_candidates, action_size)

        for t in range(T - 1):
            action_t = actions[:, t, :, :]  # Shape: (ensemble_size, n_candidates, action_size)
            delta_mean, delta_var = self.ensemble(states[t], action_t)
            if self.use_mean:
                next_state = states[t] + delta_mean
            else:
                next_state = states[t] + self.ensemble.sample(delta_mean, delta_var)
            states[t + 1] = next_state
            delta_means[t] = delta_mean
            delta_vars[t] = delta_var

        # Stack the lists into tensors
        states = torch.stack(states[1:], dim=0)  # Exclude initial state
        delta_vars = torch.stack(delta_vars, dim=0)
        delta_means = torch.stack(delta_means, dim=0)

        return states, delta_vars, delta_means
    
    # def compute_subgoal_distance_penalty(self, states, current_subgoal):
    #     """
    #     Compute a penalty based on the expected distance to the current subgoal at the end of the planning horizon.

    #     Args:
    #         states (torch.Tensor): Predicted states from rollouts, shape (plan_horizon, ensemble_size, n_candidates, state_size)
    #         current_subgoal (torch.Tensor): The current subgoal, shape (state_size,)

    #     Returns:
    #         torch.Tensor: Subgoal distance penalties for each candidate, shape (n_candidates,)
    #     """
    #     # Use the last predicted state
    #     predicted_state = states[-1]  # Shape: (ensemble_size, n_candidates, state_size)
    #     predicted_state = predicted_state.mean(dim=0)  # Mean over ensemble: (n_candidates, state_size)
    #     desired_state = current_subgoal.unsqueeze(0).repeat(self.n_candidates, 1)  # (n_candidates, state_size)

    #     # Compute distance between predicted state and current subgoal
    #     diff_subgoal = desired_state - predicted_state  # (n_candidates, state_size)
    #     distance_subgoal = torch.norm(diff_subgoal, dim=1)  # (n_candidates,)

    #     # Compute subgoal distance penalty (negative reward)
    #     subgoal_penalty = -self.subgoal_scale * distance_subgoal

    #     return subgoal_penalty


    def perform_rollout(self, current_state, actions):
        T = self.plan_horizon + 1
        states = [torch.empty(0)] * T
        delta_means = [torch.empty(0)] * (T - 1)
        delta_vars = [torch.empty(0)] * (T - 1)

        current_state = current_state.unsqueeze(dim=0).unsqueeze(dim=0)  # Shape: (1,1,state_size)
        current_state = current_state.repeat(self.ensemble_size, self.n_candidates, 1)  # Shape: (ensemble_size, n_candidates, state_size)
        states[0] = current_state

        actions = actions.unsqueeze(0)  # Shape: (1, plan_horizon, n_candidates, action_size)
        actions = actions.repeat(self.ensemble_size, 1, 1, 1).permute(1, 0, 2, 3)  # Shape: (plan_horizon, ensemble_size, n_candidates, action_size)

        for t in range(self.plan_horizon):
            delta_mean, delta_var = self.ensemble(states[t], actions[t])
            if self.use_mean:
                next_state = states[t] + delta_mean
            else:
                next_state = states[t] + self.ensemble.sample(delta_mean, delta_var)
            states[t + 1] = next_state
            delta_means[t] = delta_mean
            delta_vars[t] = delta_var

        # Stack the lists into tensors
        states = torch.stack(states[1:], dim=0)  # Exclude initial state: (plan_horizon, ensemble_size, n_candidates, state_size)
        delta_vars = torch.stack(delta_vars, dim=0)  # (plan_horizon, ensemble_size, n_candidates, state_size)
        delta_means = torch.stack(delta_means, dim=0)  # (plan_horizon, ensemble_size, n_candidates, state_size)

        return states, delta_vars, delta_means

    def compute_goal_achievement(self, states, current_subgoal):
        """
        Compute the goal achievement reward based on the distance to the subgoal at each timestep.

        Args:
            states (torch.Tensor): Predicted states from rollouts, shape (plan_horizon, ensemble_size, n_candidates, state_size)
            current_subgoal (torch.Tensor): The current subgoal, shape (state_size,)

        Returns:
            torch.Tensor: Goal achievement rewards for each candidate, shape (n_candidates,)
        """
        goal_achievements = torch.zeros(self.n_candidates).float().to(self.device)
        for t in range(self.plan_horizon):
            predicted_state = states[t]  # (ensemble_size, n_candidates, state_size)
            predicted_state = predicted_state.mean(dim=0)  # (n_candidates, state_size)
            desired_state = current_subgoal.unsqueeze(0).repeat(self.n_candidates, 1)  # (n_candidates, state_size)
            diff_subgoal = desired_state - predicted_state  # (n_candidates, state_size)
            distance_subgoal = torch.norm(diff_subgoal, dim=1)  # (n_candidates,)

            # Compute goal achievement reward
            goal_reward = -distance_subgoal
            goal_achievements += goal_reward  # Accumulate over time steps

        return goal_achievements


    # def compute_goal_achievement(self, states, goals):
    #     """
    #     Compute the goal achievement reward based on the distance to subgoals.

    #     Args:
    #         states (torch.Tensor): Predicted states from rollouts, shape (plan_horizon, ensemble_size, n_candidates, state_size)
    #         goals (torch.Tensor): Subgoals to achieve, shape (num_subgoals, state_size)

    #     Returns:
    #         torch.Tensor: Goal achievement rewards for each candidate, shape (n_candidates,)
    #     """
    #     goal_achievements = torch.zeros(self.n_candidates).float().to(self.device)
    #     c = self.context_length
    #     num_subgoals = goals.size(0)

    #     for i in range(num_subgoals):
    #         t = i * c + c - 1  # Last timestep in the context
    #         if t >= self.plan_horizon:
    #             t = self.plan_horizon - 1  # Ensure t is within bounds
    #         predicted_state = states[t]  # Shape: (ensemble_size, n_candidates, state_size)
    #         predicted_state = predicted_state.mean(dim=0)  # Mean over ensemble: (n_candidates, state_size)
    #         desired_state = goals[i].unsqueeze(0).repeat(self.n_candidates, 1)  # (n_candidates, state_size)

    #         # Distance between predicted state and subgoal
    #         diff_subgoal = desired_state - predicted_state  # (n_candidates, state_size)
    #         distance_subgoal = torch.norm(diff_subgoal, dim=1)  # (n_candidates,)

    #         # Compute goal achievement reward
    #         # Higher reward for closer distances
    #         goal_reward = -self.subgoal_scale * distance_subgoal

    #         goal_achievements += goal_reward

    #         # Logging for debugging
    #         # if self.logger is not None:
    #         #     subgoal_values = desired_state.mean(dim=0).cpu().numpy()
    #         #     self.logger.log(f"Subgoal {i+1}/{num_subgoals} values: {subgoal_values}")
    #         #     self.logger.log(f"Subgoal {i+1}/{num_subgoals} distances mean: {distance_subgoal.mean().item():.4f}")
    #         #     self.logger.log(f"Subgoal {i+1}/{num_subgoals} goal rewards mean: {goal_reward.mean().item():.4f}")
    #         #     self.logger.log("=========================================================================")

    #     return goal_achievements

    def _update_action_distribution(self, actions, returns):
        returns = torch.where(torch.isnan(returns), torch.zeros_like(returns), returns)
        _, topk = returns.topk(self.top_candidates, dim=0, largest=True, sorted=False)

        # Update action distribution
        best_actions = actions[:, topk.view(-1)].reshape(
            self.plan_horizon, self.top_candidates, self.action_size
        )
        action_mean = best_actions.mean(dim=1, keepdim=True)
        action_std_dev = best_actions.std(dim=1, unbiased=False, keepdim=True)

        return action_mean, action_std_dev

    def _update_goal_action_distribution(self, actions, returns):
        """
        Update the action distribution for subgoal selection using top candidates.

        Args:
            actions (torch.Tensor): Actions sampled for subgoal candidates, shape (context_length, n_subgoal_candidates, action_size)
            returns (torch.Tensor): Returns computed for subgoal candidates, shape (n_subgoal_candidates,)

        Returns:
            Tuple of updated action mean and std dev.
        """
        returns = torch.where(torch.isnan(returns), torch.zeros_like(returns), returns)
        _, topk = returns.topk(self.top_candidates, dim=0, largest=True, sorted=False)

        # Update action distribution
        best_actions = actions[:, topk.view(-1)].reshape(
            self.context_length, self.top_candidates, self.action_size
        )
        action_mean = best_actions.mean(dim=1, keepdim=True)
        action_std_dev = best_actions.std(dim=1, unbiased=False, keepdim=True)

        return action_mean, action_std_dev

    def return_stats(self):
        if self.use_reward:
            reward_stats = self._create_stats(self.trial_rewards)
        else:
            reward_stats = {}
        if self.use_exploration:
            info_stats = self._create_stats(self.trial_bonuses)
        else:
            info_stats = {}
        self.trial_rewards = []
        self.trial_bonuses = []
        return reward_stats, info_stats

    def _create_stats(self, arr):
        tensor = torch.stack(arr)
        tensor = tensor.view(-1)
        return {
            "max": tensor.max().item(),
            "min": tensor.min().item(),
            "mean": tensor.mean().item(),
            "std": tensor.std().item(),
        }

# End file: pmbrl/control/planner.py

==============================

==============================
# Begin file: pmbrl/control/measures.py
# pylint: disable=not-callable
# pylint: disable=no-member

import torch
import numpy as np
from scipy.special import psi, gamma


class Random(object):
    def __init__(self, model, scale=1.0):
        self.model = model
        self.scale = scale

    def __call__(self, delta_means, delta_vars):
        """
        delta_means   (plan_horizon, ensemble_size, n_candidates, n_dim)
        delta_vars    (plan_horizon, ensemble_size, n_candidates, n_dim)
        """

        n_candidates = delta_means.size(2)
        randoms = torch.randn(n_candidates).float().to(delta_means.device)

        return randoms


class Variance(object):
    def __init__(self, model, scale=1.0):
        self.model = model
        self.scale = scale

    def __call__(self, delta_means, delta_vars):
        """
        delta_means   (plan_horizon, ensemble_size, n_candidates, n_dim)
        delta_vars    (plan_horizon, ensemble_size, n_candidates, n_dim)
        """
        plan_horizon = delta_means.size(0)
        n_candidates = delta_means.size(2)
        delta_means = self.model.normalizer.renormalize_state_delta_means(delta_means)
        variance = (
            torch.zeros(plan_horizon, n_candidates).float().to(delta_means.device)
        )
        for t in range(plan_horizon):
            variance[t, :] = self.get_variance(delta_vars[t])

        variance = variance * self.scale
        return variance.sum(dim=0)

    def get_variance(self, delta_vars):
        """ensemble_size, candidates, n_dim"""
        variance = delta_vars.sum(dim=0).sum(dim=-1)
        return variance


class Disagreement(object):
    def __init__(self, model, scale=1.0):
        self.model = model
        self.scale = scale

    def __call__(self, delta_means, delta_vars):
        """
        delta_means   (plan_horizon, ensemble_size, n_candidates, n_dim)
        delta_vars    (plan_horizon, ensemble_size, n_candidates, n_dim)
        """
        plan_horizon = delta_means.size(0)
        n_candidates = delta_means.size(2)
        delta_means = self.model.normalizer.renormalize_state_delta_means(delta_means)
        disagreements = (
            torch.zeros(plan_horizon, n_candidates).float().to(delta_means.device)
        )
        for t in range(plan_horizon):
            disagreements[t, :] = self.get_disagreement(delta_means[t])

        disagreements = disagreements * self.scale
        return disagreements.sum(dim=0)

    def get_disagreement(self, delta_means):
        """ensemble_size, candidates, n_dim"""
        disagreement = delta_means.std(dim=0).sum(dim=-1)
        return disagreement


class InformationGain(object):
    def __init__(self, model, scale=1.0):
        self.model = model
        self.scale = scale

    def __call__(self, delta_means, delta_vars):
        """
        delta_means   (plan_horizon, ensemble_size, n_candidates, n_dim)
        delta_vars    (plan_horizon, ensemble_size, n_candidates, n_dim)
        """

        plan_horizon = delta_means.size(0)
        n_candidates = delta_means.size(2)

        delta_means = self.model.normalizer.renormalize_state_delta_means(delta_means)
        delta_vars = self.model.normalizer.renormalize_state_delta_vars(delta_vars)
        delta_states = self.model.sample(delta_means, delta_vars)
        info_gains = (
            torch.zeros(plan_horizon, n_candidates).float().to(delta_means.device)
        )

        for t in range(plan_horizon):
            """
                Detailed explaination of Information Gain and why our environment entropy is modeled as an ensemble model.
                https://chatgpt.com/share/66e8825f-7480-8001-b910-50fa17aa73da
                
                # TODO: For next paper, what'a a better world model to use then an ensemble model?
                How to ensure that the model we use is as true as possible to true dynamics of the environment?
                Is there a better way to represent the true dynamics of the environment?
                
                
                In practice, especially in model-based reinforcement learning and active inference frameworks, 
                the ensemble model serves as an approximation of the true dynamics of the environment. 
                
                Model Approximation:
                The ensemble represents multiple hypotheses about the environment's dynamics. 
                By aggregating these predictions, entropy_of_average captures the uncertainty modeled by the ensemble, serving as a stand-in for the true entropy H(Y).

                Ensemble as Proxy for True Distribution:
                In scenarios where the true distribution is unknown or intractable, an ensemble model provides a practical way to estimate 
                H(Y). The diversity within the ensemble captures model uncertainty, which is crucial for effective exploration.

                Y: Future state of the environment.
                X: Ensemble model's predictions based on a candidate action sequence.
                H(Y): Entropy of Y (uncertainty about without any knowledge of X).
                H(Yâˆ£X): Conditional entropy of Y given X (remaining uncertainty about Y after knowing X).
                I(Y; X): Mutual information between Y and X (reduction in uncertainty about Y after knowing X).

            """
            ent_avg = self.entropy_of_average(delta_states[t])  # Estimates H(Y) - Entropy of the -- supposed true, approximated using model itself-- dynamics of the environment
            avg_ent = self.average_of_entropy(delta_vars[t])  # Estimates H(Y | X)
            info_gains[t, :] = ent_avg - avg_ent  # Computes I(Y; X)

        info_gains = info_gains * self.scale
        return info_gains.sum(dim=0)

    def entropy_of_average(self, samples):
        """
        samples (ensemble_size, n_candidates, n_dim)
        """
        samples = samples.permute(1, 0, 2)
        n_samples = samples.size(1)
        dims = samples.size(2)
        k = 3

        distances_yy = self.batched_cdist_l2(samples, samples)
        y, _ = torch.sort(distances_yy, dim=1)
        v = self.volume_of_the_unit_ball(dims)
        h = (
            np.log(n_samples - 1)
            - psi(k)
            + np.log(v)
            + dims * torch.sum(torch.log(y[:, k - 1]), dim=1) / n_samples
            + 0.5
        )
        return h

    def batched_cdist_l2(self, x1, x2):
        x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)
        x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)
        res = (
            torch.baddbmm(x2_norm.transpose(-2, -1), x1, x2.transpose(-2, -1), alpha=-2)
            .add_(x1_norm)
            .clamp_min_(1e-30)
            .sqrt_()
        )
        return res

    def volume_of_the_unit_ball(self, dim):
        return np.pi ** (dim / 2) / gamma(dim / 2 + 1)

    def average_of_entropy(self, delta_vars):
        return torch.mean(self.gaussian_diagonal_entropy(delta_vars), dim=0)

    def gaussian_diagonal_entropy(self, delta_vars):
        min_variance = 1e-8
        return 0.5 * torch.sum(
            torch.log(2 * np.pi * np.e * torch.clamp(delta_vars, min=min_variance)),
            dim=len(delta_vars.size()) - 1,
        )


# End file: pmbrl/control/measures.py

==============================

==============================
# Begin file: pmbrl/control/agent.py
# Begin file: pmbrl/control/agent.py
from copy import deepcopy
import os

import numpy as np
import torch
from PIL import Image, ImageDraw

class Agent(object):
    def __init__(self, env, planner, logger=None):
        self.env = env
        self.planner = planner
        self.logger = logger
        self.current_goal = None  # Initialize current_goal

    def get_seed_episodes(self, buffer, n_episodes):
        for _ in range(n_episodes):
            state = self.env.reset()
            done = False
            while not done:
                action = self.env.sample_action()
                next_state, reward, done, _ = self.env.step(action)
                buffer.add(state, action, reward, next_state)
                state = deepcopy(next_state)
                if done:
                    break
        return buffer

    def run_episode(self, buffer=None, action_noise=None, recorder=None):
        total_reward = 0
        total_steps = 0
        done = False

        # Prepare folder for saving frames
        if recorder is not None:
            folder_name = os.path.splitext(recorder.path)[0]  # Remove extension from video path
            os.makedirs(folder_name, exist_ok=True)  # Create folder with the same name as the video file
                
        with torch.no_grad():
            state = self.env.reset()
            self.current_goal = self.planner.global_goal_state.cpu().numpy()  # Initialize current_goal
            while not done:
                action, current_goal = self.planner(state)  # Receive both action and current_goal
                self.current_goal = current_goal.cpu().numpy()  # Update current_goal

                if action_noise is not None:
                    action = self._add_action_noise(action, action_noise)
                action = action.cpu().detach().numpy()

                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                total_steps += 1

                if self.logger is not None and total_steps % 25 == 0:
                    self.logger.log(
                        "> Step {} [reward {:.2f}]".format(total_steps, total_reward)
                    )

                if buffer is not None:
                    buffer.add(state, action, reward, next_state)
                try:
                    if recorder is not None:
                        recorder.capture_frame()
                except AttributeError as e:
                    self.logger.log(f"AttributeError: {e}")

                state = deepcopy(next_state)
                
                self.render_and_save_frame(state, action, folder_name, total_steps)

                if done:
                    break

        if recorder is not None:
            recorder.close()
            del recorder

        self.env.close()
        stats = self.planner.return_stats()
        return total_reward, total_steps, stats

    def _add_action_noise(self, action, noise):
        if noise is not None:
            action = action + noise * torch.randn_like(action)
        return action

    def render_and_save_frame(self, state, action, folder_name, step):
        """
        Render the current frame, paint the goal and markers, and save the image.

        Args:
        state: Current state of the environment
        folder_name: Directory to save the frames
        step: Current step number
        """
        # Render the frame
        frame = self.env.unwrapped.render(mode="rgb_array")
        goal = self.current_goal
        # self.logger.log(f"goal : {goal}")

        if frame is not None:
            img = Image.fromarray(frame)
            draw = ImageDraw.Draw(img)

            # Get frame dimensions
            height, width = frame.shape[:2]

            # Correct calculation for the goal state using cos(theta) and sin(theta)
            # We assume the goal is represented in the same way as the pendulum's state: [cos(goal_theta), sin(goal_theta)]
            goal_theta = np.arctan2(goal[1], goal[0])

            # Flip the sign of goal_theta to correct the orientation
            goal_theta = -goal_theta

            # Calculate goal position in the image
            goal_x = int(width / 2 + np.sin(goal_theta) * (height / 4))  # Similar logic to the pendulum's x position
            goal_y = int(height / 2 - np.cos(goal_theta) * (height / 4))  # Similar logic to the pendulum's y position

            # Draw the goal state on the image (green)
            draw.ellipse([goal_x-5, goal_y-5, goal_x+5, goal_y+5], fill=(0, 255, 0), outline=(0, 255, 0))

            # Draw markers at (0,0) and (1000,1000) (red) - Reference points
            draw.ellipse([0-5, 0-5, 0+5, 0+5], fill=(255, 0, 0), outline=(255, 0, 0))
            draw.ellipse([1000-5, 1000-5, 1000+5, 1000+5], fill=(255, 0, 0), outline=(255, 0, 0))

            # Corrected calculation for the pendulum's free end (orange)
            # For Pendulum-v0, state[0] is cos(theta), state[1] is sin(theta)
            theta = np.arctan2(state[1], state[0])

            # Flip the sign of theta to correct the orientation
            theta = -theta

            # Calculate pendulum's free end position, bringing the orange dot closer by reducing the height scaling
            pendulum_x = int(width / 2 + np.sin(theta) * (height / 4))  # Reduced length factor from height / 3 to height / 4
            pendulum_y = int(height / 2 - np.cos(theta) * (height / 4))  # Reduced length factor here as well

            # Debugging print statements to check positions
            # print(f"Step: {step}, Pendulum position: ({pendulum_x}, {pendulum_y}), State: {state}, Action: {action}")
            # print(f'Goal position: ({goal_x}, {goal_y}), Goal state : {self.current_goal}')

            # Draw the current state (pendulum's free end) (orange)
            draw.ellipse([pendulum_x-5, pendulum_y-5, pendulum_x+5, pendulum_y+5], fill=(255, 165, 0), outline=(255, 165, 0))

            # Save the modified frame as an image
            img.save(f"{folder_name}/frame_{step}.png")
        else:
            print(f"Warning: Frame {step} could not be rendered.")
# End file: pmbrl/control/agent.py

# End file: pmbrl/control/agent.py

==============================

==============================
# Begin file: scripts/train.py
# pylint: disable=not-callable
# pylint: disable=no-member

# Begin file: scripts/train.py
import sys
import time
import pathlib
import argparse

import numpy as np
import torch
from gym.wrappers.monitoring.video_recorder import VideoRecorder

sys.path.append(str(pathlib.Path(__file__).parent.parent))

from pmbrl.envs import GymEnv
from pmbrl.training import Normalizer, Buffer, Trainer
from pmbrl.models import EnsembleModel, RewardModel
from pmbrl.control import Planner, Agent
from pmbrl.utils import Logger
from pmbrl import get_config

DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

def main(args):
    logger = Logger(args.logdir, args.seed)
    logger.log("\n=== Loading experiment [device: {}] ===\n".format(DEVICE))
    logger.log(args)

    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)

    env = GymEnv(
        args.env_name, args.max_episode_len, action_repeat=args.action_repeat, seed=args.seed
    )
    action_size = env.action_space.shape[0]
    state_size = env.observation_space.shape[0]

    normalizer = Normalizer()
    buffer = Buffer(state_size, action_size, args.ensemble_size, normalizer, device=DEVICE)

    ensemble = EnsembleModel(
        state_size + action_size,
        state_size,
        args.hidden_size,
        args.ensemble_size,
        normalizer,
        device=DEVICE,
    )
    reward_model = RewardModel(state_size + action_size, args.hidden_size, device=DEVICE)

    trainer = Trainer(
        ensemble,
        reward_model,
        buffer,
        n_train_epochs=args.n_train_epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        epsilon=args.epsilon,
        grad_clip_norm=args.grad_clip_norm,
        logger=logger,
    )

    # Define the global goal state (maximum reward state)
    global_goal_state = env.max_reward_state  # Ensure your environment has this method

    # planner = Planner(
    #     ensemble,
    #     reward_model,
    #     action_size,
    #     args.ensemble_size,
    #     plan_horizon=args.plan_horizon,
    #     optimisation_iters=args.optimisation_iters,
    #     n_candidates=args.n_candidates,
    #     top_candidates=args.top_candidates,
    #     use_reward=args.use_reward,
    #     use_exploration=args.use_exploration,
    #     use_mean=args.use_mean,
    #     expl_scale=args.expl_scale,
    #     reward_scale=args.reward_scale,
    #     strategy=args.strategy,
    #     use_high_level=True,
    #     context_length=args.context_length,
    #     goal_achievement_scale=args.goal_achievement_scale,
    #     global_goal_state=torch.tensor(global_goal_state, dtype=torch.float32).to(DEVICE),
    #     device=DEVICE,
    #     # New parameters
    #     global_goal_weight=args.global_goal_weight,
    #     max_subgoal_distance=args.max_subgoal_distance,
    #     initial_goal_std=args.initial_goal_std,
    #     goal_std_decay=args.goal_std_decay,
    #     min_goal_std=args.min_goal_std,
    #     goal_mean_weight=args.goal_mean_weight,
    # )
    
    
    planner = Planner(
        env,
        ensemble,
        reward_model,
        action_size,
        args.ensemble_size,
        plan_horizon=args.plan_horizon,
        optimisation_iters=args.optimisation_iters,
        n_candidates=args.n_candidates,
        top_candidates=args.top_candidates,
        use_reward=False,
        use_exploration=False,
        # use_reward=args.use_reward,
        # use_exploration=args.use_exploration,
        use_mean = args.use_mean,
        # use_mean=args.use_mean,
        expl_scale=args.expl_scale,
        reward_scale=args.reward_scale,
        strategy=args.strategy,
        use_high_level=True,
        context_length=args.context_length,
        goal_achievement_scale=args.goal_achievement_scale,
        global_goal_state=torch.tensor(global_goal_state, dtype=torch.float32).to(DEVICE),
        device=DEVICE,
        # New parameters
        # global_goal_weight=args.global_goal_weight,
        # max_subgoal_distance=args.max_subgoal_distance,
        # initial_goal_std=args.initial_goal_std,
        # goal_std_decay=args.goal_std_decay,
        # min_goal_std=args.min_goal_std,
        # goal_mean_weight=args.goal_mean_weight,
        # Additional parameters
        subgoal_scale=args.subgoal_scale,
        global_goal_scale=args.global_goal_scale,
        logger=logger,  # Pass the logger here
    )

    agent = Agent(env, planner, logger=logger)

    agent.get_seed_episodes(buffer, args.n_seed_episodes)
    msg = "\nCollected seeds: [{} episodes | {} frames]"
    logger.log(msg.format(args.n_seed_episodes, buffer.total_steps))

    for episode in range(1, args.n_episodes + 1):
        logger.log("\n=== Episode {} ===".format(episode))
        start_time = time.time()

        msg = "Training on [{}/{}] data points"
        logger.log(msg.format(buffer.total_steps, buffer.total_steps * args.action_repeat))
        trainer.reset_models()
        ensemble_loss, reward_loss = trainer.train()
        logger.log_losses(ensemble_loss, reward_loss)

        recorder = None
        print(f'Recording every {args.record_every} episodes, episode: {episode}')
        if args.record_every is not None and args.record_every % episode == 0:
            filename = logger.get_video_path(episode)
            recorder = VideoRecorder(env.unwrapped, path=filename)
            logger.log("Setup recorder @ {}".format(filename))

        logger.log("\n=== Collecting data [{}] ===".format(episode))
        reward, steps, stats = agent.run_episode(
                buffer, action_noise=args.action_noise, recorder=recorder
            )
        logger.log_episode(reward, steps)
        logger.log_stats(stats)

        logger.log_time(time.time() - start_time)
        logger.save()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--logdir", type=str, default="log")
    parser.add_argument("--config_name", type=str, default="mountain_car")
    parser.add_argument("--strategy", type=str, default="information")
    parser.add_argument("--seed", type=int, default=0)
    # In scripts/train.py
    # parser.add_argument("--global_goal_weight", type=float, default=1.0)
    # parser.add_argument("--max_subgoal_distance", type=float, default=7.0)
    # parser.add_argument("--initial_goal_std", type=float, default=1.0)
    # parser.add_argument("--goal_std_decay", type=float, default=0.99)
    # parser.add_argument("--min_goal_std", type=float, default=0.1)
    parser.add_argument("--expl_scale", type=float, default=1000.0)
    parser.add_argument("--reward_scale", type=float, default=1000.0)
    parser.add_argument("--use_reward", action="store_true", default=False)
    parser.add_argument("--use_exploration", action="store_true", default=False)
    
    parser.add_argument("--goal_mean_weight", type=float, default=0.8)
    parser.add_argument("--goal_achievement_scale", type=float, default=1000.0)
    parser.add_argument("--subgoal_scale", type=float, default=1.0)
    parser.add_argument("--global_goal_scale", type=float, default=1.0)
    

    args = parser.parse_args()
    config = get_config(args)
    main(config)

# End file: scripts/train.py

==============================
